{"type":"data","nodes":[null,{"type":"data","data":[{"id":1,"content":2,"html":3,"title":4,"date":5,"tags":6},"rce","{\n\t\"title\": \"Finding Deals on UberEats with Node.js and GitHub Actions\",\n\t\"date\": 1707239672851,\n\t\"tags\": [\"scraping\", \"javascript\", \"github-actions\", \"puppeteer\"]\n}\n---\n\nHow I was able to snag up UberEats buy-one-get-one-free (BOGO) deals from UberEats in my area. [TL](https://recurse-eats.dim.codes/);[DR](https://github.com/xDimGG/recurse-ubereats/).\n\n## Background\n\nI recently started my batch at [Recurse Center](https://www.recurse.com/about). Although I have been learning a lot and having a good time with all the people here, the food in this area is awfully expensive, and on some days I don't have time to prepare lunch at home. I looked at a couple of food delivery sites like DoorDash, GrubHub, etc. As it turns out, UberEats routinely offers BOGO deals at a good number of restaurants. It was very annoying to sift through all the nearby restaurants for what offers they have, so I wanted a way to quickly view all of the items that had an offer going on.\n\n![UberEats Search](/rce_1.png)\n\n## Investigation\n\nAs it turns out, UberEats doesn't really want you scraping their site. Most API endpoints have obscure names, the URL is encoded in some strange way, and the React state is quite annoyingly obfuscated. It's okay, though. As with any sort of reverse engineering, anything is possible (unless you encounter CAPTCHA, then you go on a long walk and consider whether all this is even worth it). Let's start with something simple.\n\n### The URL\n\nFirst off, we need to find restaurants in our area, so let's navigate to UberEats and search for the current address (for Recurse Center, that's 397 Bridge St). On this page, we can see restaurants in our area. Let's try and pick apart the URL.\n\n```\nhttps://www.ubereats.com/feed\n\t?diningMode=PICKUP\n\t&pl=JTdCJTIyYWRkcmVzcyUyMiUzQSUyMjM5NyUyMEJyaWRnZSUyMFN0JTIyJTJDJTIycmVmZXJlbmNlJTIyJTNBJTIyaGVyZSUzQWFmJTNBc3RyZWV0c2VjdGlvbiUzQWRrcFQwMXY0d3p1N1VEWHp3MFBvTUElM0FDZ2NJQkNEUHQtVWpFQUVhQXpNNU53JTIyJTJDJTIycmVmZXJlbmNlVHlwZSUyMiUzQSUyMmhlcmVfcGxhY2VzJTIyJTJDJTIybGF0aXR1ZGUlMjIlM0E0MC42OTEzNiUyQyUyMmxvbmdpdHVkZSUyMiUzQS03My45ODUyJTdE\n```\n\nOf course, `diningMode` is either `PICKUP` or `DELIVERY`. Since my goal here is to save money, not spend it, we are going to be using `PICKUP` and not `DELIVERY`. As for the second parameter, `pl`, it definitely seems a bit trickier. Based on my years of professional hacking, I am going to take a guess and say that this is just Base64 encoded. You can try plugging this into an online Base64 decoder or do what I do and just use [`atob`](https://developer.mozilla.org/en-US/docs/Web/API/atob) from the Chrome console.\n\n```js\n> atob('JTdCJTIyYWRkcmVzcyUyMiUzQSUyMjM5NyUyMEJyaWRnZSUyMFN0JTIyJTJDJTIycmVmZXJlbmNlJTIyJTNBJTIyaGVyZSUzQWFmJTNBc3RyZWV0c2VjdGlvbiUzQWRrcFQwMXY0d3p1N1VEWHp3MFBvTUElM0FDZ2NJQkNEUHQtVWpFQUVhQXpNNU53JTIyJTJDJTIycmVmZXJlbmNlVHlwZSUyMiUzQSUyMmhlcmVfcGxhY2VzJTIyJTJDJTIybGF0aXR1ZGUlMjIlM0E0MC42OTEzNiUyQyUyMmxvbmdpdHVkZSUyMiUzQS03My45ODUyJTdE')\n\u003C '%7B%22address%22%3A%22397%20Bridge%20St%22%2C%22reference%22%3A%22here%3Aaf%3Astreetsection%3AdkpT01v4wzu7UDXzw0PoMA%3ACgcIBCDPt-UjEAEaAzM5Nw%22%2C%22referenceType%22%3A%22here_places%22%2C%22latitude%22%3A40.69136%2C%22longitude%22%3A-73.9852%7D'\n```\n\nWell, the resulting string just looks like it's been URI encoded so let's see what happens if we call [`decodeURIComponent`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/decodeURIComponent) on it.\n\n```js\n> decodeURIComponent('%7B%22address%22%3A%22397%20Bridge%20St%22%2C%22reference%22%3A%22here%3Aaf%3Astreetsection%3AdkpT01v4wzu7UDXzw0PoMA%3ACgcIBCDPt-UjEAEaAzM5Nw%22%2C%22referenceType%22%3A%22here_places%22%2C%22latitude%22%3A40.69136%2C%22longitude%22%3A-73.9852%7D')\n\u003C {\"address\":\"397 Bridge St\",\"reference\":\"here:af:streetsection:dkpT01v4wzu7UDXzw0PoMA:CgcIBCDPt-UjEAEaAzM5Nw\",\"referenceType\":\"here_places\",\"latitude\":40.69136,\"longitude\":-73.9852}\n```\n\nHow nice, it looks like some clean and readable JSON. Let's just restructure it so we can inspect it a bit.\n\n```json\n{\n\t\"address\": \"397 Bridge St\",\n\t\"reference\": \"here:af:streetsection:dkpT01v4wzu7UDXzw0PoMA:CgcIBCDPt-UjEAEaAzM5Nw\",\n\t\"referenceType\": \"here_places\",\n\t\"latitude\": 40.69136,\n\t\"longitude\": -73.9852\n}\n```\n\nSo now, we have our decoding function,\n```js\nconst decode = pl => JSON.parse(decodeURIComponent(atob(pl)));\n```\nand inversely, our encoding function,\n```js\nconst encode = obj => btoa(encodeURIComponent(JSON.stringify(obj)));\n```\n\nAfter some testing of swapping out the latitiude, longitude, etc., it turns out that `/feed` bases its queries on the `reference` parameter. This means we can omit all the other fields and pass an encoded object that only has a `reference` field. For example:\n\n```js\n> encode({ reference: 'here:af:streetsection:dkpT01v4wzu7UDXzw0PoMA:CgcIBCDPt-UjEAEaAzM5Nw' })\n\u003C 'JTdCJTIycmVmZXJlbmNlJTIyJTNBJTIyaGVyZSUzQWFmJTNBc3RyZWV0c2VjdGlvbiUzQWRrcFQwMXY0d3p1N1VEWHp3MFBvTUElM0FDZ2NJQkNEUHQtVWpFQUVhQXpNNU53JTIyJTdE'\n```\n\nWhen plugging this string into our `pl` parameter, the results are identical. We now have a programmatic way of generating URLs for a given location. The only problem is that I don't actually know what `here:af:streetsection:dkpT01v4wzu7UDXzw0PoMA:CgcIBCDPt-UjEAEaAzM5Nw` actually means. It's not a string I've ever seen before. Taking `here:af:streetsection:` to Google, I am led to the [HERE developer docs](https://www.here.com/docs/category/geocoding-search-v7). It seems to be some kind of string representing our address. I'm sure there's some way that by interacting with the HERE API, we can generate these strings for any given input. However, since I just want to use this scraper for only one location, I'll stick with the original URL that we had in our browser. Surprise! We did all this work just not to use any of it. It happens sometimes.\n\n### The Page\n\nLet's see if we can do this the cool way without puppeteer. I start by viewing the source of the page and searching for \"moonbowls\". The reason I'm doing this is to check if the server prerenders the results and we can access them by just fetching the page's HTML.\n\n![moonbowls search](/rce_2.png)\n\nNo results. Darn. To the Chrome DevTools network tab we go. In the network tab, we may click on the search button and enter \"moonbowls\". This will search for everything in all requests (the URL, the header, the body, etc.).\n\n![moonbowls search network tab](/rce_3.png)\n\nOne hit! The page seems to be sending a POST request to `https://www.ubereats.com/_p/api/getFeedV1?localeCode=en-US`. Let's check out the payload it's sending.\n\n![getFeedV1 payload](/rce_4.png)\n\nThat's a lot of JSON fields, but most of them are empty. All but one. `cacheKey` seems to have another Base64 encoded string. (Partially omitted for clarity.)\n\n```\n\"cacheKey\": \"JTdCJTIyYWR...45ODUyJTdE/PICKUP///0/0//JTVCJTVE/undefined//////HOME///////\"\n```\n\nThe keen among you may have realized that the Base64 string is the same as the one in our URL. This means that the `cacheKey` parameter is just our `pl` parameter concatenated with `/PICKUP///0/0//JTVCJTVE/undefined//////HOME///////`. After a quick test with another address, `JTVCJTVE` remains the same. Some of the other values in between the forward slashes are filled out and some aren't. It doesn't seem to matter for our purposes, so let's just keep `/PICKUP///0/0//JTVCJTVE/undefined//////HOME///////` as a constant.\n\n### The API Request\n\nWell, we have enough information to start making queries using an HTTP library. Here, I'll be using `node-fetch` since it's my personal favorite and Chrome DevTools lets you copy the request as a Node.js fetch call.\n\n![copy as node.js fetch](/rce_5.png)\n\n```js\nfetch(\"https://www.ubereats.com/_p/api/getFeedV1?localeCode=en-US\", {\n\t// copied straight from chrome\n}).then(res => res.json()).then(console.log);\n```\n\nRunning this results in the following.\n\n```js\n{ status: 'failure', data: { message: 'bd.error.too_many_requests' } }\n```\n\nWell, that sucks. It seems like UberEats might be doing some cookie stuff to make clients use a new cookie for each request. We can try and get around this by\n\n1. Maintaining the cookies using a cookie jar, which will update our cookie state based on set-cookie response headers.\n2. Tracking the requests and responses that the browser makes, and trying to mimic them so we can have a browser-like cookie state.\n\nI did try this route for a good while before coming to the conclusion that it was very difficult and the server always seemed to figure out I was not a real browser. Well, when the server wants you to be a browser, why not just be a browser?\n\n### Puppeteer\n\n[Puppeteer](https://pptr.dev/), the greatest invention since sliced bread, is an API for controlling a headless Chrome instance. This means we can have a browser physically visit the UberEats site, load the page, and then scrape the information ourselves from the elements on the page. I typically try to avoid using puppeteer since it sort of feels like cheating and uses a lot more resources compared to a few HTTP requests. First, install puppeteer with `npm i puppeteer`. After that, get it to visit our target URL. *Note: `headless: false` makes it so that we can see the browser in action. In production, one should set this to `true` (actually `'new'` to avoid deprecation warnings).*\n\n```js\nconst feedURL = 'https://www.ubereats.com/feed?diningMode=PICKUP&pl=...';\n\nconsole.log('launching puppeteer...');\nconst browser = await puppeteer.launch({ headless: false });\nconst page = (await browser.pages())[0];\n\nconsole.log('getting nearby restaurants..');\nawait page.goto(feedURL);\n```\n\nNow that we've visited the page, let's open the Chrome element inspector and find a path to our restaurant cards.\n\n![looking at page HTML](/rce_6.png)\n\nAs it turns out, these class names are minified, which means that if UberEats were to ever deploy a new version, it would likely break our scraper since the minified filenames are highly susceptible to change. Instead, let's use a selector which is more concrete and not based on classes. *Note: the way I'm using a query selector here is by pressing CMD/CTRL+F in the Chrome Element Inspector.*\n\n![the selector that works](/rce_7.png)\n\nOne of the anchor tags inside the card has an attribute called `data-testid` which is always `\"store-card\"`. This seems pretty good and when querying the page for `a[data-testid=\"store-card\"]`, we get 80 results, which is the number we want. The selector I went with is `div:has(> div > div > div > a[data-testid=\"store-card\"])`. Since we want to select the parent div of the anchor element, we may use the [:has()](https://developer.mozilla.org/en-US/docs/Web/CSS/:has) selector to help us. Now that we have the card, we want to check if it has a green ribbon, which means that it is likely to have a BOGO deal. If it does have a green ribbon and that green ribbon contains the text `Buy 1, Get 1 Free` or `n Offers Available`, we want to store the URL of the restaurant page to be scraped later. The code we end up with looks like this. *Note: we are using `page.waitForSelector(cards)` here to wait until the cards are present on the page before scraping them.*\n\n```js\nconst cards = 'div:has(> div > div > div > a[data-testid=\"store-card\"])';\nawait page.waitForSelector(cards);\n\nconst restaurants = [];\nfor (const el of await page.$$(cards)) {\n\tconst offer = await el.evaluate(e => e.querySelector('picture + div > div')?.textContent) || '';\n\tif (offer.includes('Get 1 Free') || offer.includes('Offers')) {\n\t\trestaurants.push(await el.evaluate(e => e.querySelector('a').href));\n\t}\n}\n\nconsole.log(`${restaurants.length} potential restaurants with offers found! closing puppeteer...`);\nawait browser.close();\n```\n\n### Back to Fetch\n\nAnyway, that's all we need from puppeteer. As it turns out, there is a straightforward way to parse the restaurant menu data from the page without making any special API requests. If we go to the URL of the restaurant (ex: [moonbowls](https://www.ubereats.com/store/moonbowls-healthy-korean-bowls-bridge-st/d8CWVYDzXySpFNvEAIzWtQ)). When we view source and search for a particular item on their menu (ex: BBQ Chicken Bowl), we can find that the data is contained within the server-sent HTML. It's all in this one script tag that looks like this:\n\n```html\n\u003Cscript type=\"application/json\" id=\"__REACT_QUERY_STATE__\">\n\t{\\u0022mutations\\u0022:[],\\u0022queries\\u0022:[{ and on and on and on...\n\u003C/script>\n```\n\nLet's go ahead and get this tag from a script using the following code. *Note: it turns out that we need a \"good\" User-Agent*\n\n```js\nconst body = await fetch(url, {\n\theaders: {\n\t\t'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n\t},\n}).then(res => res.text());\nconst reactData = body.match(/__REACT_QUERY_STATE__\">(.*?)\u003C\\/script>/s)[1];\nconst rawData = JSON.parse(decodeURIComponent(JSON.parse(`\"${reactData.trim()}\"`)));\n```\n\nHere, I'm just fetching the page and parsing the context as text. Then, I run a regular expression on the text to find whatever is inside of the script tag. The data is a bit weird, but I first tried to unescape the string by wrapping it with quotes and using JSON.parse. After that, I realized it was still URI encoded so I use decodeURIComponent. Finally, we can JSON.parse that.\n\n### Cleaning and Saving the Data\n\nEverything from this point is mostly smooth sailing. Just apply this logic to every restaurant URL we have, get the fields from the JSON that are relevant to us, and save them all to a file. *Note: there is a ton of JSON fields and sifting through this is not fun.* The code we end up with looks like this.\n\n```js\nconst allCompiled = [];\nfor (let i = 0; i \u003C restaurants.length; i++) {\n\tconst url = restaurants[i];\n\tconsole.log(`(${i+1}/${restaurants.length}) fetching ${url}...`);\n\n\tconst body = await fetch(url, {\n\t\theaders: {\n\t\t\t'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n\t\t},\n\t}).then(res => res.text());\n\tconst reactData = body.match(/__REACT_QUERY_STATE__\">(.*?)\u003C\\/script>/s)[1];\n\tconst rawData = JSON.parse(decodeURIComponent(JSON.parse(`\"${reactData.trim()}\"`)));\n\tconst { data } = rawData.queries[0].state;\n\tconst [section] = data.sections;\n\tif (section.isOnSale && data.catalogSectionsMap[section.uuid]) {\n\t\tconst items = new Map();\n\t\tfor (const { payload } of data.catalogSectionsMap[section.uuid]) {\n\t\t\tfor (const item of payload.standardItemsPayload.catalogItems) {\n\t\t\t\titems.set(item.uuid, item);\n\t\t\t}\n\t\t}\n\n\t\tconst deals = [];\n\t\tfor (const item of items.values()) {\n\t\t\tif (item.itemPromotion) deals.push(item);\n\t\t}\n\n\t\t// deals found\n\t\tif (deals.length) { \n\t\t\t// formatting the json to our liking\n\t\t\tconst compiled = JSON.parse(data.metaJson);\n\t\t\tcompiled.deals = deals;\n\t\t\tdelete compiled.hasMenu;\n\n\t\t\tallCompiled.push(compiled);\n\t\t\tconsole.log(`got data for ${compiled.name}: ${deals.length} deal(s) found`);\n\t\t}\n\t}\n\n\tawait new Promise(r => setTimeout(r, 3000)); // sleep for 3 secs to avoid ratelimiting\n}\n\nfs.writeFileSync('./scraped.json', JSON.stringify(allCompiled)); // output our deals to scraped.json\n```\n\n### GitHub Action\n\nAnd that's it! I did mention that we are using a GitHub Action to automatically scrape the data so here is the script for that. Every 10 minutes, it checks out the repository, runs our JS file, and commits the changed files back to the repository. *Note: `GH_TOKEN` is a repository secret that contains a GitHub personal access token with write content permission to the repository.*\n\n```yml\nname: Cron Scrape\n\non:\n  schedule:\n    - cron: \"*/10 * * * *\"\n\npermissions:\n  contents: write\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    name: Run scrape.mjs and commit changes\n    env:\n      GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}\n    steps:\n    - uses: actions/checkout@v3\n    - name: Setup Node\n      uses: actions/setup-node@v3\n      with:\n        node-version: 20.3.0\n    - run: npm i\n    - run: |\n        git config --global user.name \"gh-actions\"\n        git config --global user.email \"gh-actions@github.com\"\n        git remote set-url origin https://git:${GITHUB_TOKEN}@github.com/${{github.repository}}.git\n    - run: npm run main\n    - run: git commit --allow-empty -am \"update scraped.json\"\n    - run: git push\n```\n\n### Rendering the Data\n\nWith the data successfully scraped, we may now make a nice user interface. I did so using GitHub Pages and you can visit it [here](https://recurse-eats.dim.codes/). The source code for the page itself is [here](https://github.com/xDimGG/recurse-ubereats/blob/main/index.html).\n\n## Final Notes\n\nI hope you can go and start using puppeteer yourself. I barely scratched the surface of puppeteer's capabilities, but these are probably the most useful parts. During this small project, I learned that it's better to quit early and opt for the \"easier\" method sooner as sometimes all you need is a working product that can feed some hungry stomachs. Hope you've enjoyed. See you again!\n","\u003Cp>How I was able to snag up UberEats buy-one-get-one-free (BOGO) deals from UberEats in my area. \u003Ca href=\"https://recurse-eats.dim.codes/\">TL\u003C/a>;\u003Ca href=\"https://github.com/xDimGG/recurse-ubereats/\">DR\u003C/a>.\u003C/p>\n\u003Ch2>Background\u003C/h2>\n\u003Cp>I recently started my batch at \u003Ca href=\"https://www.recurse.com/about\">Recurse Center\u003C/a>. Although I have been learning a lot and having a good time with all the people here, the food in this area is awfully expensive, and on some days I don&#39;t have time to prepare lunch at home. I looked at a couple of food delivery sites like DoorDash, GrubHub, etc. As it turns out, UberEats routinely offers BOGO deals at a good number of restaurants. It was very annoying to sift through all the nearby restaurants for what offers they have, so I wanted a way to quickly view all of the items that had an offer going on.\u003C/p>\n\u003Cp>\u003Cimg src=\"/rce_1.png\" alt=\"UberEats Search\">\u003C/p>\n\u003Ch2>Investigation\u003C/h2>\n\u003Cp>As it turns out, UberEats doesn&#39;t really want you scraping their site. Most API endpoints have obscure names, the URL is encoded in some strange way, and the React state is quite annoyingly obfuscated. It&#39;s okay, though. As with any sort of reverse engineering, anything is possible (unless you encounter CAPTCHA, then you go on a long walk and consider whether all this is even worth it). Let&#39;s start with something simple.\u003C/p>\n\u003Ch3>The URL\u003C/h3>\n\u003Cp>First off, we need to find restaurants in our area, so let&#39;s navigate to UberEats and search for the current address (for Recurse Center, that&#39;s 397 Bridge St). On this page, we can see restaurants in our area. Let&#39;s try and pick apart the URL.\u003C/p>\n\u003Cpre>\u003Ccode>https://www.ubereats.com/feed\n    ?diningMode=PICKUP\n    &amp;pl=JTdCJTIyYWRkcmVzcyUyMiUzQSUyMjM5NyUyMEJyaWRnZSUyMFN0JTIyJTJDJTIycmVmZXJlbmNlJTIyJTNBJTIyaGVyZSUzQWFmJTNBc3RyZWV0c2VjdGlvbiUzQWRrcFQwMXY0d3p1N1VEWHp3MFBvTUElM0FDZ2NJQkNEUHQtVWpFQUVhQXpNNU53JTIyJTJDJTIycmVmZXJlbmNlVHlwZSUyMiUzQSUyMmhlcmVfcGxhY2VzJTIyJTJDJTIybGF0aXR1ZGUlMjIlM0E0MC42OTEzNiUyQyUyMmxvbmdpdHVkZSUyMiUzQS03My45ODUyJTdE\n\u003C/code>\u003C/pre>\u003Cp>Of course, \u003Ccode>diningMode\u003C/code> is either \u003Ccode>PICKUP\u003C/code> or \u003Ccode>DELIVERY\u003C/code>. Since my goal here is to save money, not spend it, we are going to be using \u003Ccode>PICKUP\u003C/code> and not \u003Ccode>DELIVERY\u003C/code>. As for the second parameter, \u003Ccode>pl\u003C/code>, it definitely seems a bit trickier. Based on my years of professional hacking, I am going to take a guess and say that this is just Base64 encoded. You can try plugging this into an online Base64 decoder or do what I do and just use \u003Ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/atob\">\u003Ccode>atob\u003C/code>\u003C/a> from the Chrome console.\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">&gt; \u003Cspan class=\"hljs-title function_\">atob\u003C/span>(\u003Cspan class=\"hljs-string\">&#x27;JTdCJTIyYWRkcmVzcyUyMiUzQSUyMjM5NyUyMEJyaWRnZSUyMFN0JTIyJTJDJTIycmVmZXJlbmNlJTIyJTNBJTIyaGVyZSUzQWFmJTNBc3RyZWV0c2VjdGlvbiUzQWRrcFQwMXY0d3p1N1VEWHp3MFBvTUElM0FDZ2NJQkNEUHQtVWpFQUVhQXpNNU53JTIyJTJDJTIycmVmZXJlbmNlVHlwZSUyMiUzQSUyMmhlcmVfcGxhY2VzJTIyJTJDJTIybGF0aXR1ZGUlMjIlM0E0MC42OTEzNiUyQyUyMmxvbmdpdHVkZSUyMiUzQS03My45ODUyJTdE&#x27;\u003C/span>)\n&lt; \u003Cspan class=\"hljs-string\">&#x27;%7B%22address%22%3A%22397%20Bridge%20St%22%2C%22reference%22%3A%22here%3Aaf%3Astreetsection%3AdkpT01v4wzu7UDXzw0PoMA%3ACgcIBCDPt-UjEAEaAzM5Nw%22%2C%22referenceType%22%3A%22here_places%22%2C%22latitude%22%3A40.69136%2C%22longitude%22%3A-73.9852%7D&#x27;\u003C/span>\n\u003C/code>\u003C/pre>\u003Cp>Well, the resulting string just looks like it&#39;s been URI encoded so let&#39;s see what happens if we call \u003Ca href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/decodeURIComponent\">\u003Ccode>decodeURIComponent\u003C/code>\u003C/a> on it.\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">&gt; \u003Cspan class=\"hljs-built_in\">decodeURIComponent\u003C/span>(\u003Cspan class=\"hljs-string\">&#x27;%7B%22address%22%3A%22397%20Bridge%20St%22%2C%22reference%22%3A%22here%3Aaf%3Astreetsection%3AdkpT01v4wzu7UDXzw0PoMA%3ACgcIBCDPt-UjEAEaAzM5Nw%22%2C%22referenceType%22%3A%22here_places%22%2C%22latitude%22%3A40.69136%2C%22longitude%22%3A-73.9852%7D&#x27;\u003C/span>)\n&lt; {\u003Cspan class=\"hljs-string\">&quot;address&quot;\u003C/span>:\u003Cspan class=\"hljs-string\">&quot;397 Bridge St&quot;\u003C/span>,\u003Cspan class=\"hljs-string\">&quot;reference&quot;\u003C/span>:\u003Cspan class=\"hljs-string\">&quot;here:af:streetsection:dkpT01v4wzu7UDXzw0PoMA:CgcIBCDPt-UjEAEaAzM5Nw&quot;\u003C/span>,\u003Cspan class=\"hljs-string\">&quot;referenceType&quot;\u003C/span>:\u003Cspan class=\"hljs-string\">&quot;here_places&quot;\u003C/span>,\u003Cspan class=\"hljs-string\">&quot;latitude&quot;\u003C/span>:\u003Cspan class=\"hljs-number\">40.69136\u003C/span>,\u003Cspan class=\"hljs-string\">&quot;longitude&quot;\u003C/span>:-\u003Cspan class=\"hljs-number\">73.9852\u003C/span>}\n\u003C/code>\u003C/pre>\u003Cp>How nice, it looks like some clean and readable JSON. Let&#39;s just restructure it so we can inspect it a bit.\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-json\">\u003Cspan class=\"hljs-punctuation\">{\u003C/span>\n    \u003Cspan class=\"hljs-attr\">&quot;address&quot;\u003C/span>\u003Cspan class=\"hljs-punctuation\">:\u003C/span> \u003Cspan class=\"hljs-string\">&quot;397 Bridge St&quot;\u003C/span>\u003Cspan class=\"hljs-punctuation\">,\u003C/span>\n    \u003Cspan class=\"hljs-attr\">&quot;reference&quot;\u003C/span>\u003Cspan class=\"hljs-punctuation\">:\u003C/span> \u003Cspan class=\"hljs-string\">&quot;here:af:streetsection:dkpT01v4wzu7UDXzw0PoMA:CgcIBCDPt-UjEAEaAzM5Nw&quot;\u003C/span>\u003Cspan class=\"hljs-punctuation\">,\u003C/span>\n    \u003Cspan class=\"hljs-attr\">&quot;referenceType&quot;\u003C/span>\u003Cspan class=\"hljs-punctuation\">:\u003C/span> \u003Cspan class=\"hljs-string\">&quot;here_places&quot;\u003C/span>\u003Cspan class=\"hljs-punctuation\">,\u003C/span>\n    \u003Cspan class=\"hljs-attr\">&quot;latitude&quot;\u003C/span>\u003Cspan class=\"hljs-punctuation\">:\u003C/span> \u003Cspan class=\"hljs-number\">40.69136\u003C/span>\u003Cspan class=\"hljs-punctuation\">,\u003C/span>\n    \u003Cspan class=\"hljs-attr\">&quot;longitude&quot;\u003C/span>\u003Cspan class=\"hljs-punctuation\">:\u003C/span> \u003Cspan class=\"hljs-number\">-73.9852\u003C/span>\n\u003Cspan class=\"hljs-punctuation\">}\u003C/span>\n\u003C/code>\u003C/pre>\u003Cp>So now, we have our decoding function,\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">\u003Cspan class=\"hljs-keyword\">const\u003C/span> \u003Cspan class=\"hljs-title function_\">decode\u003C/span> = pl =&gt; \u003Cspan class=\"hljs-title class_\">JSON\u003C/span>.\u003Cspan class=\"hljs-title function_\">parse\u003C/span>(\u003Cspan class=\"hljs-built_in\">decodeURIComponent\u003C/span>(\u003Cspan class=\"hljs-title function_\">atob\u003C/span>(pl)));\n\u003C/code>\u003C/pre>\u003Cp>and inversely, our encoding function,\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">\u003Cspan class=\"hljs-keyword\">const\u003C/span> \u003Cspan class=\"hljs-title function_\">encode\u003C/span> = obj =&gt; \u003Cspan class=\"hljs-title function_\">btoa\u003C/span>(\u003Cspan class=\"hljs-built_in\">encodeURIComponent\u003C/span>(\u003Cspan class=\"hljs-title class_\">JSON\u003C/span>.\u003Cspan class=\"hljs-title function_\">stringify\u003C/span>(obj)));\n\u003C/code>\u003C/pre>\u003Cp>After some testing of swapping out the latitiude, longitude, etc., it turns out that \u003Ccode>/feed\u003C/code> bases its queries on the \u003Ccode>reference\u003C/code> parameter. This means we can omit all the other fields and pass an encoded object that only has a \u003Ccode>reference\u003C/code> field. For example:\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">&gt; \u003Cspan class=\"hljs-title function_\">encode\u003C/span>({ \u003Cspan class=\"hljs-attr\">reference\u003C/span>: \u003Cspan class=\"hljs-string\">&#x27;here:af:streetsection:dkpT01v4wzu7UDXzw0PoMA:CgcIBCDPt-UjEAEaAzM5Nw&#x27;\u003C/span> })\n&lt; \u003Cspan class=\"hljs-string\">&#x27;JTdCJTIycmVmZXJlbmNlJTIyJTNBJTIyaGVyZSUzQWFmJTNBc3RyZWV0c2VjdGlvbiUzQWRrcFQwMXY0d3p1N1VEWHp3MFBvTUElM0FDZ2NJQkNEUHQtVWpFQUVhQXpNNU53JTIyJTdE&#x27;\u003C/span>\n\u003C/code>\u003C/pre>\u003Cp>When plugging this string into our \u003Ccode>pl\u003C/code> parameter, the results are identical. We now have a programmatic way of generating URLs for a given location. The only problem is that I don&#39;t actually know what \u003Ccode>here:af:streetsection:dkpT01v4wzu7UDXzw0PoMA:CgcIBCDPt-UjEAEaAzM5Nw\u003C/code> actually means. It&#39;s not a string I&#39;ve ever seen before. Taking \u003Ccode>here:af:streetsection:\u003C/code> to Google, I am led to the \u003Ca href=\"https://www.here.com/docs/category/geocoding-search-v7\">HERE developer docs\u003C/a>. It seems to be some kind of string representing our address. I&#39;m sure there&#39;s some way that by interacting with the HERE API, we can generate these strings for any given input. However, since I just want to use this scraper for only one location, I&#39;ll stick with the original URL that we had in our browser. Surprise! We did all this work just not to use any of it. It happens sometimes.\u003C/p>\n\u003Ch3>The Page\u003C/h3>\n\u003Cp>Let&#39;s see if we can do this the cool way without puppeteer. I start by viewing the source of the page and searching for &quot;moonbowls&quot;. The reason I&#39;m doing this is to check if the server prerenders the results and we can access them by just fetching the page&#39;s HTML.\u003C/p>\n\u003Cp>\u003Cimg src=\"/rce_2.png\" alt=\"moonbowls search\">\u003C/p>\n\u003Cp>No results. Darn. To the Chrome DevTools network tab we go. In the network tab, we may click on the search button and enter &quot;moonbowls&quot;. This will search for everything in all requests (the URL, the header, the body, etc.).\u003C/p>\n\u003Cp>\u003Cimg src=\"/rce_3.png\" alt=\"moonbowls search network tab\">\u003C/p>\n\u003Cp>One hit! The page seems to be sending a POST request to \u003Ccode>https://www.ubereats.com/_p/api/getFeedV1?localeCode=en-US\u003C/code>. Let&#39;s check out the payload it&#39;s sending.\u003C/p>\n\u003Cp>\u003Cimg src=\"/rce_4.png\" alt=\"getFeedV1 payload\">\u003C/p>\n\u003Cp>That&#39;s a lot of JSON fields, but most of them are empty. All but one. \u003Ccode>cacheKey\u003C/code> seems to have another Base64 encoded string. (Partially omitted for clarity.)\u003C/p>\n\u003Cpre>\u003Ccode>&quot;cacheKey&quot;: &quot;JTdCJTIyYWR...45ODUyJTdE/PICKUP///0/0//JTVCJTVE/undefined//////HOME///////&quot;\n\u003C/code>\u003C/pre>\u003Cp>The keen among you may have realized that the Base64 string is the same as the one in our URL. This means that the \u003Ccode>cacheKey\u003C/code> parameter is just our \u003Ccode>pl\u003C/code> parameter concatenated with \u003Ccode>/PICKUP///0/0//JTVCJTVE/undefined//////HOME///////\u003C/code>. After a quick test with another address, \u003Ccode>JTVCJTVE\u003C/code> remains the same. Some of the other values in between the forward slashes are filled out and some aren&#39;t. It doesn&#39;t seem to matter for our purposes, so let&#39;s just keep \u003Ccode>/PICKUP///0/0//JTVCJTVE/undefined//////HOME///////\u003C/code> as a constant.\u003C/p>\n\u003Ch3>The API Request\u003C/h3>\n\u003Cp>Well, we have enough information to start making queries using an HTTP library. Here, I&#39;ll be using \u003Ccode>node-fetch\u003C/code> since it&#39;s my personal favorite and Chrome DevTools lets you copy the request as a Node.js fetch call.\u003C/p>\n\u003Cp>\u003Cimg src=\"/rce_5.png\" alt=\"copy as node.js fetch\">\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">\u003Cspan class=\"hljs-title function_\">fetch\u003C/span>(\u003Cspan class=\"hljs-string\">&quot;https://www.ubereats.com/_p/api/getFeedV1?localeCode=en-US&quot;\u003C/span>, {\n    \u003Cspan class=\"hljs-comment\">// copied straight from chrome\u003C/span>\n}).\u003Cspan class=\"hljs-title function_\">then\u003C/span>(\u003Cspan class=\"hljs-function\">\u003Cspan class=\"hljs-params\">res\u003C/span> =&gt;\u003C/span> res.\u003Cspan class=\"hljs-title function_\">json\u003C/span>()).\u003Cspan class=\"hljs-title function_\">then\u003C/span>(\u003Cspan class=\"hljs-variable language_\">console\u003C/span>.\u003Cspan class=\"hljs-property\">log\u003C/span>);\n\u003C/code>\u003C/pre>\u003Cp>Running this results in the following.\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">{ \u003Cspan class=\"hljs-attr\">status\u003C/span>: \u003Cspan class=\"hljs-string\">&#x27;failure&#x27;\u003C/span>, \u003Cspan class=\"hljs-attr\">data\u003C/span>: { \u003Cspan class=\"hljs-attr\">message\u003C/span>: \u003Cspan class=\"hljs-string\">&#x27;bd.error.too_many_requests&#x27;\u003C/span> } }\n\u003C/code>\u003C/pre>\u003Cp>Well, that sucks. It seems like UberEats might be doing some cookie stuff to make clients use a new cookie for each request. We can try and get around this by\u003C/p>\n\u003Col>\n\u003Cli>Maintaining the cookies using a cookie jar, which will update our cookie state based on set-cookie response headers.\u003C/li>\n\u003Cli>Tracking the requests and responses that the browser makes, and trying to mimic them so we can have a browser-like cookie state.\u003C/li>\n\u003C/ol>\n\u003Cp>I did try this route for a good while before coming to the conclusion that it was very difficult and the server always seemed to figure out I was not a real browser. Well, when the server wants you to be a browser, why not just be a browser?\u003C/p>\n\u003Ch3>Puppeteer\u003C/h3>\n\u003Cp>\u003Ca href=\"https://pptr.dev/\">Puppeteer\u003C/a>, the greatest invention since sliced bread, is an API for controlling a headless Chrome instance. This means we can have a browser physically visit the UberEats site, load the page, and then scrape the information ourselves from the elements on the page. I typically try to avoid using puppeteer since it sort of feels like cheating and uses a lot more resources compared to a few HTTP requests. First, install puppeteer with \u003Ccode>npm i puppeteer\u003C/code>. After that, get it to visit our target URL. \u003Cem>Note: \u003Ccode>headless: false\u003C/code> makes it so that we can see the browser in action. In production, one should set this to \u003Ccode>true\u003C/code> (actually \u003Ccode>&#39;new&#39;\u003C/code> to avoid deprecation warnings).\u003C/em>\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">\u003Cspan class=\"hljs-keyword\">const\u003C/span> feedURL = \u003Cspan class=\"hljs-string\">&#x27;https://www.ubereats.com/feed?diningMode=PICKUP&amp;pl=...&#x27;\u003C/span>;\n\n\u003Cspan class=\"hljs-variable language_\">console\u003C/span>.\u003Cspan class=\"hljs-title function_\">log\u003C/span>(\u003Cspan class=\"hljs-string\">&#x27;launching puppeteer...&#x27;\u003C/span>);\n\u003Cspan class=\"hljs-keyword\">const\u003C/span> browser = \u003Cspan class=\"hljs-keyword\">await\u003C/span> puppeteer.\u003Cspan class=\"hljs-title function_\">launch\u003C/span>({ \u003Cspan class=\"hljs-attr\">headless\u003C/span>: \u003Cspan class=\"hljs-literal\">false\u003C/span> });\n\u003Cspan class=\"hljs-keyword\">const\u003C/span> page = (\u003Cspan class=\"hljs-keyword\">await\u003C/span> browser.\u003Cspan class=\"hljs-title function_\">pages\u003C/span>())[\u003Cspan class=\"hljs-number\">0\u003C/span>];\n\n\u003Cspan class=\"hljs-variable language_\">console\u003C/span>.\u003Cspan class=\"hljs-title function_\">log\u003C/span>(\u003Cspan class=\"hljs-string\">&#x27;getting nearby restaurants..&#x27;\u003C/span>);\n\u003Cspan class=\"hljs-keyword\">await\u003C/span> page.\u003Cspan class=\"hljs-title function_\">goto\u003C/span>(feedURL);\n\u003C/code>\u003C/pre>\u003Cp>Now that we&#39;ve visited the page, let&#39;s open the Chrome element inspector and find a path to our restaurant cards.\u003C/p>\n\u003Cp>\u003Cimg src=\"/rce_6.png\" alt=\"looking at page HTML\">\u003C/p>\n\u003Cp>As it turns out, these class names are minified, which means that if UberEats were to ever deploy a new version, it would likely break our scraper since the minified filenames are highly susceptible to change. Instead, let&#39;s use a selector which is more concrete and not based on classes. \u003Cem>Note: the way I&#39;m using a query selector here is by pressing CMD/CTRL+F in the Chrome Element Inspector.\u003C/em>\u003C/p>\n\u003Cp>\u003Cimg src=\"/rce_7.png\" alt=\"the selector that works\">\u003C/p>\n\u003Cp>One of the anchor tags inside the card has an attribute called \u003Ccode>data-testid\u003C/code> which is always \u003Ccode>&quot;store-card&quot;\u003C/code>. This seems pretty good and when querying the page for \u003Ccode>a[data-testid=&quot;store-card&quot;]\u003C/code>, we get 80 results, which is the number we want. The selector I went with is \u003Ccode>div:has(&gt; div &gt; div &gt; div &gt; a[data-testid=&quot;store-card&quot;])\u003C/code>. Since we want to select the parent div of the anchor element, we may use the \u003Ca href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/:has\">:has()\u003C/a> selector to help us. Now that we have the card, we want to check if it has a green ribbon, which means that it is likely to have a BOGO deal. If it does have a green ribbon and that green ribbon contains the text \u003Ccode>Buy 1, Get 1 Free\u003C/code> or \u003Ccode>n Offers Available\u003C/code>, we want to store the URL of the restaurant page to be scraped later. The code we end up with looks like this. \u003Cem>Note: we are using \u003Ccode>page.waitForSelector(cards)\u003C/code> here to wait until the cards are present on the page before scraping them.\u003C/em>\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">\u003Cspan class=\"hljs-keyword\">const\u003C/span> cards = \u003Cspan class=\"hljs-string\">&#x27;div:has(&gt; div &gt; div &gt; div &gt; a[data-testid=&quot;store-card&quot;])&#x27;\u003C/span>;\n\u003Cspan class=\"hljs-keyword\">await\u003C/span> page.\u003Cspan class=\"hljs-title function_\">waitForSelector\u003C/span>(cards);\n\n\u003Cspan class=\"hljs-keyword\">const\u003C/span> restaurants = [];\n\u003Cspan class=\"hljs-keyword\">for\u003C/span> (\u003Cspan class=\"hljs-keyword\">const\u003C/span> el \u003Cspan class=\"hljs-keyword\">of\u003C/span> \u003Cspan class=\"hljs-keyword\">await\u003C/span> page.$$(cards)) {\n    \u003Cspan class=\"hljs-keyword\">const\u003C/span> offer = \u003Cspan class=\"hljs-keyword\">await\u003C/span> el.evaluate(\u003Cspan class=\"hljs-function\">\u003Cspan class=\"hljs-params\">e\u003C/span> =&gt;\u003C/span> e.\u003Cspan class=\"hljs-title function_\">querySelector\u003C/span>(\u003Cspan class=\"hljs-string\">&#x27;picture + div &gt; div&#x27;\u003C/span>)?.\u003Cspan class=\"hljs-property\">textContent\u003C/span>) || \u003Cspan class=\"hljs-string\">&#x27;&#x27;\u003C/span>;\n    \u003Cspan class=\"hljs-keyword\">if\u003C/span> (offer.\u003Cspan class=\"hljs-title function_\">includes\u003C/span>(\u003Cspan class=\"hljs-string\">&#x27;Get 1 Free&#x27;\u003C/span>) || offer.\u003Cspan class=\"hljs-title function_\">includes\u003C/span>(\u003Cspan class=\"hljs-string\">&#x27;Offers&#x27;\u003C/span>)) {\n        restaurants.\u003Cspan class=\"hljs-title function_\">push\u003C/span>(\u003Cspan class=\"hljs-keyword\">await\u003C/span> el.evaluate(\u003Cspan class=\"hljs-function\">\u003Cspan class=\"hljs-params\">e\u003C/span> =&gt;\u003C/span> e.\u003Cspan class=\"hljs-title function_\">querySelector\u003C/span>(\u003Cspan class=\"hljs-string\">&#x27;a&#x27;\u003C/span>).\u003Cspan class=\"hljs-property\">href\u003C/span>));\n    }\n}\n\n\u003Cspan class=\"hljs-variable language_\">console\u003C/span>.\u003Cspan class=\"hljs-title function_\">log\u003C/span>(\u003Cspan class=\"hljs-string\">`\u003Cspan class=\"hljs-subst\">${restaurants.length}\u003C/span> potential restaurants with offers found! closing puppeteer...`\u003C/span>);\n\u003Cspan class=\"hljs-keyword\">await\u003C/span> browser.\u003Cspan class=\"hljs-title function_\">close\u003C/span>();\n\u003C/code>\u003C/pre>\u003Ch3>Back to Fetch\u003C/h3>\n\u003Cp>Anyway, that&#39;s all we need from puppeteer. As it turns out, there is a straightforward way to parse the restaurant menu data from the page without making any special API requests. If we go to the URL of the restaurant (ex: \u003Ca href=\"https://www.ubereats.com/store/moonbowls-healthy-korean-bowls-bridge-st/d8CWVYDzXySpFNvEAIzWtQ\">moonbowls\u003C/a>). When we view source and search for a particular item on their menu (ex: BBQ Chicken Bowl), we can find that the data is contained within the server-sent HTML. It&#39;s all in this one script tag that looks like this:\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-html\">\u003Cspan class=\"hljs-tag\">&lt;\u003Cspan class=\"hljs-name\">script\u003C/span> \u003Cspan class=\"hljs-attr\">type\u003C/span>=\u003Cspan class=\"hljs-string\">&quot;application/json&quot;\u003C/span> \u003Cspan class=\"hljs-attr\">id\u003C/span>=\u003Cspan class=\"hljs-string\">&quot;__REACT_QUERY_STATE__&quot;\u003C/span>&gt;\u003C/span>\n    {\\u0022mutations\\u0022:[],\\u0022queries\\u0022:[{ and on and on and on...\n\u003Cspan class=\"hljs-tag\">&lt;/\u003Cspan class=\"hljs-name\">script\u003C/span>&gt;\u003C/span>\n\u003C/code>\u003C/pre>\u003Cp>Let&#39;s go ahead and get this tag from a script using the following code. \u003Cem>Note: it turns out that we need a &quot;good&quot; User-Agent\u003C/em>\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">\u003Cspan class=\"hljs-keyword\">const\u003C/span> body = \u003Cspan class=\"hljs-keyword\">await\u003C/span> \u003Cspan class=\"hljs-title function_\">fetch\u003C/span>(url, {\n    \u003Cspan class=\"hljs-attr\">headers\u003C/span>: {\n        \u003Cspan class=\"hljs-string\">&#x27;user-agent&#x27;\u003C/span>: \u003Cspan class=\"hljs-string\">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36&#x27;\u003C/span>,\n    },\n}).\u003Cspan class=\"hljs-title function_\">then\u003C/span>(\u003Cspan class=\"hljs-function\">\u003Cspan class=\"hljs-params\">res\u003C/span> =&gt;\u003C/span> res.\u003Cspan class=\"hljs-title function_\">text\u003C/span>());\n\u003Cspan class=\"hljs-keyword\">const\u003C/span> reactData = body.\u003Cspan class=\"hljs-title function_\">match\u003C/span>(\u003Cspan class=\"hljs-regexp\">/__REACT_QUERY_STATE__&quot;&gt;(.*?)&lt;\\/script&gt;/\u003C/span>s)[\u003Cspan class=\"hljs-number\">1\u003C/span>];\n\u003Cspan class=\"hljs-keyword\">const\u003C/span> rawData = \u003Cspan class=\"hljs-title class_\">JSON\u003C/span>.\u003Cspan class=\"hljs-title function_\">parse\u003C/span>(\u003Cspan class=\"hljs-built_in\">decodeURIComponent\u003C/span>(\u003Cspan class=\"hljs-title class_\">JSON\u003C/span>.\u003Cspan class=\"hljs-title function_\">parse\u003C/span>(\u003Cspan class=\"hljs-string\">`&quot;\u003Cspan class=\"hljs-subst\">${reactData.trim()}\u003C/span>&quot;`\u003C/span>)));\n\u003C/code>\u003C/pre>\u003Cp>Here, I&#39;m just fetching the page and parsing the context as text. Then, I run a regular expression on the text to find whatever is inside of the script tag. The data is a bit weird, but I first tried to unescape the string by wrapping it with quotes and using JSON.parse. After that, I realized it was still URI encoded so I use decodeURIComponent. Finally, we can JSON.parse that.\u003C/p>\n\u003Ch3>Cleaning and Saving the Data\u003C/h3>\n\u003Cp>Everything from this point is mostly smooth sailing. Just apply this logic to every restaurant URL we have, get the fields from the JSON that are relevant to us, and save them all to a file. \u003Cem>Note: there is a ton of JSON fields and sifting through this is not fun.\u003C/em> The code we end up with looks like this.\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-js\">\u003Cspan class=\"hljs-keyword\">const\u003C/span> allCompiled = [];\n\u003Cspan class=\"hljs-keyword\">for\u003C/span> (\u003Cspan class=\"hljs-keyword\">let\u003C/span> i = \u003Cspan class=\"hljs-number\">0\u003C/span>; i &lt; restaurants.\u003Cspan class=\"hljs-property\">length\u003C/span>; i++) {\n    \u003Cspan class=\"hljs-keyword\">const\u003C/span> url = restaurants[i];\n    \u003Cspan class=\"hljs-variable language_\">console\u003C/span>.\u003Cspan class=\"hljs-title function_\">log\u003C/span>(\u003Cspan class=\"hljs-string\">`(\u003Cspan class=\"hljs-subst\">${i+\u003Cspan class=\"hljs-number\">1\u003C/span>}\u003C/span>/\u003Cspan class=\"hljs-subst\">${restaurants.length}\u003C/span>) fetching \u003Cspan class=\"hljs-subst\">${url}\u003C/span>...`\u003C/span>);\n\n    \u003Cspan class=\"hljs-keyword\">const\u003C/span> body = \u003Cspan class=\"hljs-keyword\">await\u003C/span> \u003Cspan class=\"hljs-title function_\">fetch\u003C/span>(url, {\n        \u003Cspan class=\"hljs-attr\">headers\u003C/span>: {\n            \u003Cspan class=\"hljs-string\">&#x27;user-agent&#x27;\u003C/span>: \u003Cspan class=\"hljs-string\">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36&#x27;\u003C/span>,\n        },\n    }).\u003Cspan class=\"hljs-title function_\">then\u003C/span>(\u003Cspan class=\"hljs-function\">\u003Cspan class=\"hljs-params\">res\u003C/span> =&gt;\u003C/span> res.\u003Cspan class=\"hljs-title function_\">text\u003C/span>());\n    \u003Cspan class=\"hljs-keyword\">const\u003C/span> reactData = body.\u003Cspan class=\"hljs-title function_\">match\u003C/span>(\u003Cspan class=\"hljs-regexp\">/__REACT_QUERY_STATE__&quot;&gt;(.*?)&lt;\\/script&gt;/\u003C/span>s)[\u003Cspan class=\"hljs-number\">1\u003C/span>];\n    \u003Cspan class=\"hljs-keyword\">const\u003C/span> rawData = \u003Cspan class=\"hljs-title class_\">JSON\u003C/span>.\u003Cspan class=\"hljs-title function_\">parse\u003C/span>(\u003Cspan class=\"hljs-built_in\">decodeURIComponent\u003C/span>(\u003Cspan class=\"hljs-title class_\">JSON\u003C/span>.\u003Cspan class=\"hljs-title function_\">parse\u003C/span>(\u003Cspan class=\"hljs-string\">`&quot;\u003Cspan class=\"hljs-subst\">${reactData.trim()}\u003C/span>&quot;`\u003C/span>)));\n    \u003Cspan class=\"hljs-keyword\">const\u003C/span> { data } = rawData.\u003Cspan class=\"hljs-property\">queries\u003C/span>[\u003Cspan class=\"hljs-number\">0\u003C/span>].\u003Cspan class=\"hljs-property\">state\u003C/span>;\n    \u003Cspan class=\"hljs-keyword\">const\u003C/span> [section] = data.\u003Cspan class=\"hljs-property\">sections\u003C/span>;\n    \u003Cspan class=\"hljs-keyword\">if\u003C/span> (section.\u003Cspan class=\"hljs-property\">isOnSale\u003C/span> &amp;&amp; data.\u003Cspan class=\"hljs-property\">catalogSectionsMap\u003C/span>[section.\u003Cspan class=\"hljs-property\">uuid\u003C/span>]) {\n        \u003Cspan class=\"hljs-keyword\">const\u003C/span> items = \u003Cspan class=\"hljs-keyword\">new\u003C/span> \u003Cspan class=\"hljs-title class_\">Map\u003C/span>();\n        \u003Cspan class=\"hljs-keyword\">for\u003C/span> (\u003Cspan class=\"hljs-keyword\">const\u003C/span> { payload } \u003Cspan class=\"hljs-keyword\">of\u003C/span> data.\u003Cspan class=\"hljs-property\">catalogSectionsMap\u003C/span>[section.\u003Cspan class=\"hljs-property\">uuid\u003C/span>]) {\n            \u003Cspan class=\"hljs-keyword\">for\u003C/span> (\u003Cspan class=\"hljs-keyword\">const\u003C/span> item \u003Cspan class=\"hljs-keyword\">of\u003C/span> payload.\u003Cspan class=\"hljs-property\">standardItemsPayload\u003C/span>.\u003Cspan class=\"hljs-property\">catalogItems\u003C/span>) {\n                items.\u003Cspan class=\"hljs-title function_\">set\u003C/span>(item.\u003Cspan class=\"hljs-property\">uuid\u003C/span>, item);\n            }\n        }\n\n        \u003Cspan class=\"hljs-keyword\">const\u003C/span> deals = [];\n        \u003Cspan class=\"hljs-keyword\">for\u003C/span> (\u003Cspan class=\"hljs-keyword\">const\u003C/span> item \u003Cspan class=\"hljs-keyword\">of\u003C/span> items.\u003Cspan class=\"hljs-title function_\">values\u003C/span>()) {\n            \u003Cspan class=\"hljs-keyword\">if\u003C/span> (item.\u003Cspan class=\"hljs-property\">itemPromotion\u003C/span>) deals.\u003Cspan class=\"hljs-title function_\">push\u003C/span>(item);\n        }\n\n        \u003Cspan class=\"hljs-comment\">// deals found\u003C/span>\n        \u003Cspan class=\"hljs-keyword\">if\u003C/span> (deals.\u003Cspan class=\"hljs-property\">length\u003C/span>) { \n            \u003Cspan class=\"hljs-comment\">// formatting the json to our liking\u003C/span>\n            \u003Cspan class=\"hljs-keyword\">const\u003C/span> compiled = \u003Cspan class=\"hljs-title class_\">JSON\u003C/span>.\u003Cspan class=\"hljs-title function_\">parse\u003C/span>(data.\u003Cspan class=\"hljs-property\">metaJson\u003C/span>);\n            compiled.\u003Cspan class=\"hljs-property\">deals\u003C/span> = deals;\n            \u003Cspan class=\"hljs-keyword\">delete\u003C/span> compiled.\u003Cspan class=\"hljs-property\">hasMenu\u003C/span>;\n\n            allCompiled.\u003Cspan class=\"hljs-title function_\">push\u003C/span>(compiled);\n            \u003Cspan class=\"hljs-variable language_\">console\u003C/span>.\u003Cspan class=\"hljs-title function_\">log\u003C/span>(\u003Cspan class=\"hljs-string\">`got data for \u003Cspan class=\"hljs-subst\">${compiled.name}\u003C/span>: \u003Cspan class=\"hljs-subst\">${deals.length}\u003C/span> deal(s) found`\u003C/span>);\n        }\n    }\n\n    \u003Cspan class=\"hljs-keyword\">await\u003C/span> \u003Cspan class=\"hljs-keyword\">new\u003C/span> \u003Cspan class=\"hljs-title class_\">Promise\u003C/span>(\u003Cspan class=\"hljs-function\">\u003Cspan class=\"hljs-params\">r\u003C/span> =&gt;\u003C/span> \u003Cspan class=\"hljs-built_in\">setTimeout\u003C/span>(r, \u003Cspan class=\"hljs-number\">3000\u003C/span>)); \u003Cspan class=\"hljs-comment\">// sleep for 3 secs to avoid ratelimiting\u003C/span>\n}\n\nfs.\u003Cspan class=\"hljs-title function_\">writeFileSync\u003C/span>(\u003Cspan class=\"hljs-string\">&#x27;./scraped.json&#x27;\u003C/span>, \u003Cspan class=\"hljs-title class_\">JSON\u003C/span>.\u003Cspan class=\"hljs-title function_\">stringify\u003C/span>(allCompiled)); \u003Cspan class=\"hljs-comment\">// output our deals to scraped.json\u003C/span>\n\u003C/code>\u003C/pre>\u003Ch3>GitHub Action\u003C/h3>\n\u003Cp>And that&#39;s it! I did mention that we are using a GitHub Action to automatically scrape the data so here is the script for that. Every 10 minutes, it checks out the repository, runs our JS file, and commits the changed files back to the repository. \u003Cem>Note: \u003Ccode>GH_TOKEN\u003C/code> is a repository secret that contains a GitHub personal access token with write content permission to the repository.\u003C/em>\u003C/p>\n\u003Cpre>\u003Ccode class=\"hljs language-yml\">\u003Cspan class=\"hljs-attr\">name:\u003C/span> \u003Cspan class=\"hljs-string\">Cron\u003C/span> \u003Cspan class=\"hljs-string\">Scrape\u003C/span>\n\n\u003Cspan class=\"hljs-attr\">on:\u003C/span>\n  \u003Cspan class=\"hljs-attr\">schedule:\u003C/span>\n    \u003Cspan class=\"hljs-bullet\">-\u003C/span> \u003Cspan class=\"hljs-attr\">cron:\u003C/span> \u003Cspan class=\"hljs-string\">&quot;*/10 * * * *&quot;\u003C/span>\n\n\u003Cspan class=\"hljs-attr\">permissions:\u003C/span>\n  \u003Cspan class=\"hljs-attr\">contents:\u003C/span> \u003Cspan class=\"hljs-string\">write\u003C/span>\n\n\u003Cspan class=\"hljs-attr\">jobs:\u003C/span>\n  \u003Cspan class=\"hljs-attr\">build:\u003C/span>\n    \u003Cspan class=\"hljs-attr\">runs-on:\u003C/span> \u003Cspan class=\"hljs-string\">ubuntu-latest\u003C/span>\n    \u003Cspan class=\"hljs-attr\">name:\u003C/span> \u003Cspan class=\"hljs-string\">Run\u003C/span> \u003Cspan class=\"hljs-string\">scrape.mjs\u003C/span> \u003Cspan class=\"hljs-string\">and\u003C/span> \u003Cspan class=\"hljs-string\">commit\u003C/span> \u003Cspan class=\"hljs-string\">changes\u003C/span>\n    \u003Cspan class=\"hljs-attr\">env:\u003C/span>\n      \u003Cspan class=\"hljs-attr\">GITHUB_TOKEN:\u003C/span> \u003Cspan class=\"hljs-string\">${{\u003C/span> \u003Cspan class=\"hljs-string\">secrets.GH_TOKEN\u003C/span> \u003Cspan class=\"hljs-string\">}}\u003C/span>\n    \u003Cspan class=\"hljs-attr\">steps:\u003C/span>\n    \u003Cspan class=\"hljs-bullet\">-\u003C/span> \u003Cspan class=\"hljs-attr\">uses:\u003C/span> \u003Cspan class=\"hljs-string\">actions/checkout@v3\u003C/span>\n    \u003Cspan class=\"hljs-bullet\">-\u003C/span> \u003Cspan class=\"hljs-attr\">name:\u003C/span> \u003Cspan class=\"hljs-string\">Setup\u003C/span> \u003Cspan class=\"hljs-string\">Node\u003C/span>\n      \u003Cspan class=\"hljs-attr\">uses:\u003C/span> \u003Cspan class=\"hljs-string\">actions/setup-node@v3\u003C/span>\n      \u003Cspan class=\"hljs-attr\">with:\u003C/span>\n        \u003Cspan class=\"hljs-attr\">node-version:\u003C/span> \u003Cspan class=\"hljs-number\">20.3\u003C/span>\u003Cspan class=\"hljs-number\">.0\u003C/span>\n    \u003Cspan class=\"hljs-bullet\">-\u003C/span> \u003Cspan class=\"hljs-attr\">run:\u003C/span> \u003Cspan class=\"hljs-string\">npm\u003C/span> \u003Cspan class=\"hljs-string\">i\u003C/span>\n    \u003Cspan class=\"hljs-bullet\">-\u003C/span> \u003Cspan class=\"hljs-attr\">run:\u003C/span> \u003Cspan class=\"hljs-string\">|\n        git config --global user.name &quot;gh-actions&quot;\n        git config --global user.email &quot;gh-actions@github.com&quot;\n        git remote set-url origin https://git:${GITHUB_TOKEN}@github.com/${{github.repository}}.git\n\u003C/span>    \u003Cspan class=\"hljs-bullet\">-\u003C/span> \u003Cspan class=\"hljs-attr\">run:\u003C/span> \u003Cspan class=\"hljs-string\">npm\u003C/span> \u003Cspan class=\"hljs-string\">run\u003C/span> \u003Cspan class=\"hljs-string\">main\u003C/span>\n    \u003Cspan class=\"hljs-bullet\">-\u003C/span> \u003Cspan class=\"hljs-attr\">run:\u003C/span> \u003Cspan class=\"hljs-string\">git\u003C/span> \u003Cspan class=\"hljs-string\">commit\u003C/span> \u003Cspan class=\"hljs-string\">--allow-empty\u003C/span> \u003Cspan class=\"hljs-string\">-am\u003C/span> \u003Cspan class=\"hljs-string\">&quot;update scraped.json&quot;\u003C/span>\n    \u003Cspan class=\"hljs-bullet\">-\u003C/span> \u003Cspan class=\"hljs-attr\">run:\u003C/span> \u003Cspan class=\"hljs-string\">git\u003C/span> \u003Cspan class=\"hljs-string\">push\u003C/span>\n\u003C/code>\u003C/pre>\u003Ch3>Rendering the Data\u003C/h3>\n\u003Cp>With the data successfully scraped, we may now make a nice user interface. I did so using GitHub Pages and you can visit it \u003Ca href=\"https://recurse-eats.dim.codes/\">here\u003C/a>. The source code for the page itself is \u003Ca href=\"https://github.com/xDimGG/recurse-ubereats/blob/main/index.html\">here\u003C/a>.\u003C/p>\n\u003Ch2>Final Notes\u003C/h2>\n\u003Cp>I hope you can go and start using puppeteer yourself. I barely scratched the surface of puppeteer&#39;s capabilities, but these are probably the most useful parts. During this small project, I learned that it&#39;s better to quit early and opt for the &quot;easier&quot; method sooner as sometimes all you need is a working product that can feed some hungry stomachs. Hope you&#39;ve enjoyed. See you again!\u003C/p>\n","Finding Deals on UberEats with Node.js and GitHub Actions",1707239672851,[7,8,9,10],"github-actions","javascript","puppeteer","scraping"],"uses":{"params":["id"]}}]}
